# gpt_mcp_gradio_autoload.py
import os, json, uuid, requests
import gradio as gr
from dotenv import load_dotenv
from openai import OpenAI

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1) í™˜ê²½ ë³€ìˆ˜ & í´ë¼ì´ì–¸íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

MCP_URL = "http://localhost:8000/mcp"
session_id: str | None = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2) ì„¸ì…˜ ì´ˆê¸°í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def init_session() -> str:
    payload = {
        "jsonrpc": "2.0",
        "id": f"init-{uuid.uuid4()}",
        "method": "initialize",
        "params": {
            "protocolVersion": 1,
            "capabilities": {"tools": True},
            "clientInfo": {"name": "gradio-client", "version": "0.1"}
        }
    }
    headers = {
        "Accept": "application/json, text/event-stream",
        "Content-Type": "application/json"
    }
    r = requests.post(MCP_URL, json=payload, headers=headers, timeout=10)
    r.raise_for_status()
    sid = r.headers.get("Mcp-Session-Id")
    if not sid:
        raise RuntimeError("ì„¸ì…˜ IDë¥¼ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸ”‘ ì„¸ì…˜ í™•ë³´: {sid}")

    init_done = {
        "jsonrpc": "2.0",
        "method": "notifications/initialized",
        "params": {}
    }
    headers["Mcp-Session-Id"] = sid
    requests.post(MCP_URL, json=init_done, headers=headers, timeout=5).raise_for_status()

    return sid

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3) GPT + MCP ëŒ€í™” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat_with_tool(user_input: str):
    global session_id
    if session_id is None:
        session_id = init_session()

    messages = [
        {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¼ë°˜ ëŒ€í™”ì— ë‹µë³€í•˜ë‹¤ê°€, â€˜í”„ë¡œì íŠ¸â€™Â·â€˜ë°ì´í„°ë² ì´ìŠ¤â€™ "
                "ê°™ì€ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ë©´ MCP ë„êµ¬(search_databases ë“±)ë¥¼ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤."
            )
        },
        {"role": "user", "content": user_input}
    ]

    projects_cache = None
    chat_kwargs = {
        "model": "gpt-4o-mini",
        "messages": messages,
        "tool_choice": "auto",
        "tools": [
            {
                "type": "function",
                "function": {
                    "name": "search_databases",
                    "description": "Notionì—ì„œ ì‚¬ìš©ì ì§ˆì˜ì— ë§ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ê°ì²´(ìŠ¤í‚¤ë§ˆ) ëª©ë¡ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "query": {"type": "string"}
                        },
                        "required": ["query"]
                    },
                    "returns": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {"type": "string"},
                          "title": {"type": "string"}
                        },
                        "required": ["id", "title"]
                      }
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_projects",
                    "description": "íŠ¹ì • database_idì— ì†í•œ ëª¨ë“  í”„ë¡œì íŠ¸ í•­ëª©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "database_id": {"type": "string"}
                        },
                        "required": ["database_id"]
                    },
                    "returns": {
                        "type": "object",
                        "properties": {
                            "projects": {
                                "type": "array",
                                "items": {"type": "object"}
                            }
                        }
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_database_schema",
                    "description": "íŠ¹ì • database_idì˜ ì†ì„± ì˜µì…˜(ìŠ¤í‚¤ë§ˆ) ì „ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "get_schema": {"type": "string"}
                        },
                        "required": ["get_schema"]
                    },
                    "returns": {
                        "type": "object",
                        "properties": {
                            "schema": {
                                "type": "object",
                                "additionalProperties": {
                                    "type": "array",
                                    "items": {"type": "string"}
                                }
                            }
                        }
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "analyze_projects",
                    "description": (
                        "ì¡°íšŒëœ í”„ë¡œì íŠ¸ ëª©ë¡ì„ ë°›ì•„, ì•„ë˜ **ë¶„ì„ ë°©ì‹**ì— ë”°ë¼ ê²°ê³¼ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:\n\n"
                        "â‘  ì •ëŸ‰ ë¶„ì„ (ë°ì´í„° ê¸°ë°˜ í†µê³„)\n"
                        "- ì´ í”„ë¡œì íŠ¸ ê°œìˆ˜, ì™„ë£Œ/ì§„í–‰/ë³´ë¥˜ ë¹„ìœ¨\n"
                        "- í‰ê·  ì™„ë£Œìœ¨, ì „ì²´ ì™„ë£Œëœ íƒœìŠ¤í¬ ë¹„ìœ¨\n"
                        "- í‰ê·  ì†Œìš” ì‹œê°„(ì¢…ë£Œì¼â€“ì‹œì‘ì¼), ë§ˆê° ì´ˆê³¼ìœ¨, ì§€ì—° í”„ë¡œì íŠ¸ ë¹„ìœ¨\n"
                        "- ìš°ì„ ìˆœìœ„(High/Medium/Low) ë¶„í¬\n"
                        "- ë‹´ë‹¹ìë³„ í”„ë¡œì íŠ¸ ìˆ˜ ë° ì—…ë¬´ í¸ì¤‘ ì—¬ë¶€\n"
                        "- íŒ€(íƒœê·¸)ë³„ í”„ë¡œì íŠ¸ ë¶„í¬\n\n"
                        "â‘¡ ì •ì„± ë¶„ì„ (ë‚´ìš© ê¸°ë°˜)\n"
                        "- í”„ë¡œì íŠ¸ ì„¤ëª… í…ìŠ¤íŠ¸ ìš”ì•½\n"
                        "- â€˜ì§€ì—°â€™, â€˜ë¬¸ì œâ€™, â€˜ë§‰í˜â€™ í‚¤ì›Œë“œ íƒìƒ‰ìœ¼ë¡œ ë¦¬ìŠ¤í¬ íƒì§€\n"
                        "- ë¹„íš¨ìœ¨ì ì¸ ì¼ì •Â·êµ¬ì¡° ê°œì„  ì œì•ˆ\n"
                        "- í”„ë¡œì íŠ¸ ê°„ ì—°ê´€ì„± ë§¤í•‘ (Relation ê¸°ë°˜)\n\n"
                        "ê²°ê³¼ëŠ” **ì˜ˆì‹œ í˜•íƒœ**ë¡œ ì•„ë˜ì²˜ëŸ¼ ì¶œë ¥í•˜ì„¸ìš”:\n"
                        "```\n"
                        "ì´ í”„ë¡œì íŠ¸ ìˆ˜: 35ê°œ (ì™„ë£Œ 20, ì§„í–‰ 10, ë³´ë¥˜ 5)\n"
                        "í‰ê·  ì†Œìš” ê¸°ê°„: 12.4ì¼ (í‰ê·  ë§ˆê° ì´ˆê³¼: +3.2ì¼)\n"
                        "High ìš°ì„ ìˆœìœ„ ì¤‘ 40% ì§€ì—°ë¨\n\n"
                        "ğŸ’¬ ê°œì„  ì œì•ˆ:\n"
                        "- ë§ˆê°ì¼ ì´ˆê³¼ ê±´ å¤š â†’ ì¤‘ê°„ ì ê²€ ì£¼ê¸° ë„ì…\n"
                        "- â€˜ë§ˆì¼€íŒ…â€™ íŒ€ í”„ë¡œì íŠ¸ ì§„í–‰ë¥  ë‚®ìŒ â†’ ì¸ë ¥ ì¬ë°°ì¹˜ ê²€í† \n"
                        "```"
                    ),
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "analyze": {
                                "type": "array",
                                "items": {"type": "object"}
                            }
                        },
                        "required": ["analyze"]
                    },
                    "returns": {
                        "type": "object",
                        "properties": {
                            "analysis": {"type": "string"}
                        }
                    }
                }
            },
            {
                "type": "function",
                "function": {
                    "name": "get_page_summary",
                    "description": "í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "page_id": {"type": "string", "description": "í˜ì´ì§€ ID"}
                        },
                        "required": ["page_id"]
                    }
                }
            }
        ]
    }

    while True:
        res = client.chat.completions.create(**chat_kwargs)
        ai_msg = res.choices[0].message

        # ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ ë°”ë¡œ ì‘ë‹µ ì¢…ë£Œ
        if not ai_msg.tool_calls:
            return ai_msg.content

        # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
        for call in ai_msg.tool_calls:
            fn_name = call.function.name
            fn_args = json.loads(call.function.arguments or "{}")

            # analyze_projects í˜¸ì¶œ ì‹œ cached projects ì£¼ì…
            if fn_name == "analyze_projects" and "analyze" not in fn_args:
                # â‘  ìºì‹œê°€ ë¹„ì–´ ìˆìœ¼ë©´ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ â†’ í”„ë¡œì íŠ¸ ë¡œë“œ
                if not projects_cache:
                    # 1) ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰
                    search_payload = {
                        "jsonrpc":"2.0","id":str(uuid.uuid4()),
                        "method":"tools/call",
                        "params":{"name":"search_databases","arguments":{"query":"í”„ë¡œì íŠ¸"}}
                    }
                    resp_db = requests.post(MCP_URL, json=search_payload, headers=headers, timeout=10)
                    parsed_db = parse_mcp_response(resp_db)
                    if isinstance(parsed_db, list):
                        db_list = parsed_db
                    elif isinstance(parsed_db, dict):
                        db_list = parsed_db.get("result") or parsed_db.get("databases") or []
                    else:
                        db_list = []

                    if not db_list:
                        return "âš ï¸ â€˜í”„ë¡œì íŠ¸â€™ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

                    db_id = db_list[0].get("id")
                    if not db_id:
                        return "âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ IDë¥¼ íŒŒì‹±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

                    # 2) í”„ë¡œì íŠ¸ ëª©ë¡ ì¡°íšŒ
                    proj_payload = {
                        "jsonrpc":"2.0","id":str(uuid.uuid4()),
                        "method":"tools/call",
                        "params":{"name":"get_projects","arguments":{"database_id":db_id}}
                    }
                    resp_proj = requests.post(MCP_URL, json=proj_payload, headers=headers, timeout=10)
                    projects_cache = parse_mcp_response(resp_proj).get("projects", [])
                    if not projects_cache:
                        return "âš ï¸ ì¡°íšŒëœ í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

                # 3) ë¶„ì„ ì¸ìë¡œ ì£¼ì…
                fn_args["analyze"] = projects_cache

            # MCP í˜¸ì¶œ ì¤€ë¹„
            payload = {
                "jsonrpc": "2.0",
                "id": str(uuid.uuid4()),
                "method": "tools/call",
                "params": {"name": fn_name, "arguments": fn_args}
            }
            headers = {
                "Accept": "application/json, text/event-stream",
                "Content-Type": "application/json; charset=utf-8",
                "Mcp-Session-Id": session_id
            }

            print(f"\n[ğŸ“¡ MCP í˜¸ì¶œ] {fn_name} {fn_args}")
            resp = requests.post(MCP_URL, json=payload, headers=headers, timeout=30)
            print(f"[âœ… status] {resp.status_code}")
            print(f"[ğŸ“¦ body ] {resp.text}")
            resp.raise_for_status()

            tool_output = parse_mcp_response(resp)

            if fn_name == "get_projects":
                projects_cache = tool_output.get("projects")

            # GPTì— ê²°ê³¼ ì „ë‹¬
            messages += [
                {"role": "assistant", "content": None, "tool_calls": [call.model_dump()]},
                {
                    "role": "tool",
                    "tool_call_id": call.id,
                    "name": fn_name,
                    "content": json.dumps(tool_output, ensure_ascii=False, separators=(",", ":"))
                }
            ]
        chat_kwargs["messages"] = messages


def parse_mcp_response(resp):
    """
    FastMCP SSE ë˜ëŠ” JSON ì‘ë‹µì„ dict   â† (ë˜ëŠ” list) ë¡œ ë³€í™˜
    ë‚´ë¶€ text ë¸”ë¡ë„ ìë™ìœ¼ë¡œ json.loads()
    """
    # 1) ìŠ¤íŠ¸ë¦¼ / JSON ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° â”€â”€â”€â”€â”€â”€
    if resp.headers.get("Content-Type", "").startswith("text/event-stream"):
        resp.encoding = "utf-8"
        data_lines = []
        for line in resp.iter_lines(decode_unicode=True):
            if line == "":                     # ì´ë²¤íŠ¸ ì¢…ë£Œ
                break
            if line.startswith("data:"):
                data_lines.append(line[5:].lstrip())
        raw = "\n".join(data_lines)
        outer = json.loads(raw, strict=False)
    else:
        resp.encoding = "utf-8"
        outer = resp.json()

    # 2) FastMCP result.content[0].text ì¶”ì¶œ â”€â”€â”€â”€â”€â”€
    if isinstance(outer, dict) and "result" in outer:
        content = outer["result"].get("content", [])
        if content and isinstance(content[0], dict):
            text_block = content[0].get("text")
            if text_block:
                try:
                    return json.loads(text_block, strict=False)  # ì‹¤ì œ íˆ´ ë°˜í™˜ê°’
                except json.JSONDecodeError:
                    pass  # text ê°€ ë‹¨ìˆœ ë¬¸ìì—´ì´ë¼ë©´ ê·¸ëŒ€ë¡œ ë‘ 
        # Fallback: result ìì²´ ë°˜í™˜
        return outer["result"]

    return outer   # ì´ë¯¸ dict/list í˜•íƒœ


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4) Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with gr.Blocks() as demo:
    gr.Markdown("## ğŸ§  GPT + Notion MCP ì¸í„°í˜ì´ìŠ¤")
    txt = gr.Textbox(label="ë©”ì‹œì§€", placeholder="ì˜ˆ: í”„ë¡œì íŠ¸ ëª©ë¡ ë³´ì—¬ì¤˜")
    out = gr.Textbox(label="ì‘ë‹µ", lines=12)
    txt.submit(chat_with_tool, inputs=txt, outputs=out)

if __name__ == "__main__":
    demo.launch()
